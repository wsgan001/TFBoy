### 博客收集

看这方面的文章陌生名词很多，基本上遇到陌生的都会查一下这些名词讲的是什么，有什么作用。但又因为这些东西看一遍是记不住的，所以做一份分类收集。


#### 模型相关

一个思路清晰的讲义:[对线性回归，logistic回归和一般回归的认识](http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html)
这是一篇翻译的神经网络入门教程：[Softmax分类函数](http://www.jianshu.com/p/8eb17fa41164)
[Softmax 函数的特点和作用是什么？](https://www.zhihu.com/question/23765351)

> 概念简单介绍：
> 一般来说，回归不用在分类问题上，因为回归是连续型模型，而且受噪声影响比较大。如果非要应用进入，可以使用`logistic`回归。其实`logistic`就是在本身的线性模型基础上加上了`sigmoid`函数（神经网络那一节提到的，代替阶跃函数的`sigmoid`），使结果分布到0,1上。`Softmax`是基于二分类的`logistic`回归模型实现的多分类回归模型。

### 数学相关
[如何通俗地理解概率论中的“极大似然估计法”?](https://www.zhihu.com/question/24124998)

### Tensorflow
[TensorFlow固定图的权重并储存为Protocol Buffers](https://www.ouyangsong.com/2017/05/23/tensorflow-freeze-model-protocolbuffers/)

### 理论
[深度机器学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)
[Meaning of an Epoch in Neural Networks Training](https://stackoverflow.com/questions/31155388/meaning-of-an-epoch-in-neural-networks-training)  
> Epoch 可以理解为对一组数据训练循环次数， batch size 理解为一次训练放入多少数据